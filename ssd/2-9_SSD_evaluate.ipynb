{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48f0d59f",
   "metadata": {},
   "source": [
    "## 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b86233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5906c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content\n",
    "!git clone https://github.com/rs1004/research.git\n",
    "%cd /content/research/ssd\n",
    "%mkdir data\n",
    "%cd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c0ab29",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q /content/drive/MyDrive/data/VOCdevkit.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec327c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/research/ssd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46dfb2b",
   "metadata": {},
   "source": [
    "## 評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e540a6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils.ssd_model import DataTransform\n",
    "import json\n",
    "from utils.ssd_model import SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62667a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalDataset(Dataset):\n",
    "    def __init__(self, transform):\n",
    "        self.image_list = self._get_image_list()\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_info = self.image_list[idx]\n",
    "        id = image_info['id']\n",
    "        file_name = image_info['file_name']\n",
    "        image_file_path = './data/VOCdevkit/val/' + file_name\n",
    "        img = cv2.imread(image_file_path)[..., ::-1]  # [高さ][幅][色BGR]\n",
    "        height, width, channels = img.shape  # 画像のサイズを取得\n",
    "\n",
    "        # 4. 前処理\n",
    "        phase = \"val\"\n",
    "        img_transformed, boxes, labels = self.transform(\n",
    "            img, phase, \"\", \"\")  # アノテーションはないので、\"\"にする\n",
    "        img = torch.from_numpy(img_transformed).permute(2, 0, 1)\n",
    "        \n",
    "        return img, torch.tensor([id, height, width])\n",
    "    \n",
    "    def _get_image_list(self):\n",
    "        with open('./data/VOCdevkit/instances_val.json') as f:\n",
    "            image_list = json.load(f)['images']\n",
    "        \n",
    "        return image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e608acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_mean = (123, 117, 104)  # (BGR)の色の平均値\n",
    "# color_std = (58, 57, 57)\n",
    "color_std = None\n",
    "input_size = 300 \n",
    "\n",
    "dataset = EvalDataset(\n",
    "    DataTransform(input_size, color_mean, color_std)\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311b8cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_classes = ['aeroplane', 'bicycle', 'bird', 'boat',\n",
    "               'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "               'cow', 'diningtable', 'dog', 'horse',\n",
    "               'motorbike', 'person', 'pottedplant',\n",
    "               'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "# SSD300の設定\n",
    "ssd_cfg = {\n",
    "    'num_classes': 21,  # 背景クラスを含めた合計クラス数\n",
    "    'input_size': 300,  # 画像の入力サイズ\n",
    "    'bbox_aspect_num': [4, 6, 6, 6, 4, 4],  # 出力するDBoxのアスペクト比の種類\n",
    "    'feature_maps': [38, 19, 10, 5, 3, 1],  # 各sourceの画像サイズ\n",
    "    'steps': [8, 16, 32, 64, 100, 300],  # DBOXの大きさを決める\n",
    "    'min_sizes': [30, 60, 111, 162, 213, 264],  # DBOXの大きさを決める\n",
    "    'max_sizes': [60, 111, 162, 213, 264, 315],  # DBOXの大きさを決める\n",
    "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "}\n",
    "\n",
    "# SSDネットワークモデル\n",
    "net = SSD(phase=\"inference\", cfg=ssd_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc97eaf6",
   "metadata": {},
   "source": [
    "## 評価実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ff0b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from tqdm import tqdm\n",
    "from utils.ssd_model import Detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fdb32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "net.cuda()\n",
    "\n",
    "detect = Detect(conf_thresh=0.1)\n",
    "\n",
    "for weights_path in Path('/content/drive/MyDrive/weights/std').glob('ssd300*.pth'):\n",
    "    net_weights = torch.load(weights_path, map_location='cpu')\n",
    "    net.load_state_dict(net_weights)\n",
    "    \n",
    "    ret = []\n",
    "    print(weights_path.as_posix())\n",
    "    for images, metas in dataloader:\n",
    "        images = images.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = net(images)\n",
    "            detections = detect(outputs[0], outputs[1], outputs[2].cuda())\n",
    "            for detection, meta in zip(detections, metas):\n",
    "                id, height, width = meta\n",
    "                for class_id, row in enumerate(detection):\n",
    "                    for score, xmin, ymin, xmax, ymax in row:\n",
    "                        if score >= 0.1:\n",
    "                            ret.append({\n",
    "                                'image_id': id.item(),\n",
    "                                'category_id': class_id,\n",
    "                                'bbox': [\n",
    "                                    xmin.item() * width.item(),\n",
    "                                    ymin.item() * height.item(),\n",
    "                                    (xmax - xmin).item() * width.item(),\n",
    "                                    (ymax - ymin).item() * height.item()\n",
    "                                ],\n",
    "                                'score': score.item()\n",
    "                            })\n",
    "    cocoGt = COCO('./data/VOCdevkit/instances_val.json')\n",
    "    cocoDt = cocoGt.loadRes(ret)\n",
    "    cocoEval = COCOeval(cocoGt, cocoDt, 'bbox')\n",
    "    cocoEval.evaluate()\n",
    "    cocoEval.accumulate()\n",
    "    cocoEval.summarize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
